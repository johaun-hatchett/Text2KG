{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import logging\n",
    "\n",
    "openai.organization = 'org-ZSjLwLfwPKSKiv1oL1veHDLb'\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "logging.basicConfig(filename='responses.log', \n",
    "                    filemode='w', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(levelname)s:%(asctime)s %(message)s', \n",
    "                    datefmt='%m-%d-%Y %H:%M:%S')\n",
    "\n",
    "# GPT prompt-response wrapper\n",
    "def GPT(messages: list) -> str:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        temperature=0.3,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    logging.info(messages)\n",
    "    logging.info(reply)\n",
    "\n",
    "    return reply"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gathering Wiki Pages**\n",
    "\n",
    "Extract text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi as wiki_api\n",
    "\n",
    "wiki = wiki_api.Wikipedia('en')\n",
    "\n",
    "def extract_wiki_text(sections: wiki_api.WikipediaPageSection, blacklist: set={'See also', 'References', 'Notes', 'External links', 'Bibliography', 'Further reading'}, sec_num=None) -> dict:\n",
    "    \"\"\"Recursively extracts text from sections and subsections.\"\"\"\n",
    "    # omit acknowledgements, further reading, etc.\n",
    "    valid_sections = {s.title for s in sections}^blacklist\n",
    "    passage = {}\n",
    "\n",
    "    for s in sections:\n",
    "        num = sections.index(s)+1\n",
    "        if s.title in valid_sections:\n",
    "            if sec_num:\n",
    "                uid = f'{sec_num}.{num}'\n",
    "            else:\n",
    "                uid = f'{num}'\n",
    "            passage.update({uid: {\"title\": s.title, \"text\":s.text}})\n",
    "\n",
    "        p = extract_wiki_text(s.sections, blacklist, sec_num=uid)\n",
    "        passage.update(p)\n",
    "    \n",
    "    return passage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge graph construction**\n",
    "\n",
    "Prompt GPT-3.5 to extract keywords and triplets from passage sequentially. The format of the output varies, so there are a few exception catches:\n",
    "\n",
    "1. The output is a markdown-formatted JSON\n",
    "2. The output is JSON as-is\n",
    "3. The output is an unformatted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_info(passage: str) -> tuple:\n",
    "    \"\"\"Extract keywords and triplets from passage.\"\"\"\n",
    "    context = (f'This is an information extraction task; only perform the assigned tasks'\n",
    "               f' and adhere to the given formatting. Suppress all other outputs.'\n",
    "               f' Use the following passage as context: {passage}')\n",
    "    \n",
    "    task1 = (f'Extract the named entities from the passage. The output should be'\n",
    "             f' a JSON-formatted list of key terms, ordered such that the most important'\n",
    "             f' term is first, and the least important term is last. Example: [\"atom\", \"proton\", ...]')\n",
    "    \n",
    "    task2 = (f'Using these terms and the passage, generate knowledge graph triplets.'\n",
    "             f' Output should be a JSON-formatted list of strings of the form:' \n",
    "             f' [\"subject|relationship|object\", \"subject|relation|object\", ...].')\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": task1}\n",
    "    ]\n",
    "\n",
    "    response = GPT(messages)\n",
    "    \n",
    "    try:\n",
    "        # remove markdown formatting\n",
    "        json_str = re.sub(r'^```[a-z]+\\n(.+?)\\n```$', r'\\1', response, flags=re.DOTALL)\n",
    "        keywords = json.loads(json_str)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        try:\n",
    "            keywords = json.loads(response)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": task1},\n",
    "        {\"role\": \"assistant\", \"content\": response},\n",
    "        {\"role\": \"user\", \"content\": task2}\n",
    "    ]\n",
    "\n",
    "    response = GPT(messages)\n",
    "    \n",
    "    try:\n",
    "        # remove markdown formatting\n",
    "        json_str = re.sub(r'^```[a-z]+\\n(.+?)\\n```$', r'\\1', response, flags=re.DOTALL)\n",
    "        triplets = json.loads(json_str)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        try:\n",
    "            triplets = json.loads(response)\n",
    "        except:\n",
    "            try:\n",
    "                triplets = [item.strip(',') for item in response.splitlines() if item]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return keywords, triplets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End-to-End Process**\n",
    "\n",
    "Input:\n",
    "- list of Wikipedia pages\n",
    "\n",
    "Output:\n",
    "- JSON file containing the following hierarchy: `corpus_index/page_data/sections/information`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [07:35<00:00, 14.24s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#CORPUS = [wiki.page(title) for title in ['Atom', 'Proton', 'Neutron', 'Electron', 'Atomic_Nucleus']]\n",
    "CORPUS = [wiki.page('Atom')]\n",
    "\n",
    "data = [{\"page_title\": page.title, \"content\": extract_wiki_text(page.sections)} for page in CORPUS]\n",
    "\n",
    "filename = 'corpus.json'\n",
    "filepath = os.path.join('data/original', filename)\n",
    "    \n",
    "for page in data:\n",
    "    for section in tqdm(page[\"content\"]):\n",
    "        if page[\"content\"][section][\"text\"]:\n",
    "            try:\n",
    "                keywords, triplets = extract_info(page[\"content\"][section][\"text\"])\n",
    "            except:\n",
    "                print(f\"Encountered an issue in section {section} of page {page['page_title']}\")\n",
    "                \n",
    "            page[\"content\"][section].update({\"keywords\": keywords, \"triplets\": triplets})\n",
    "\n",
    "with open(filepath, \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'data.csv'\n",
    "filepath = os.path.join('data/original',filename) \n",
    "\n",
    "def write_triplets_to_csv(section: dict, page_id: str, sec_id: str) -> list:\n",
    "     errors = []\n",
    "     \n",
    "     if \"triplets\" in section:\n",
    "          for triplet in section[\"triplets\"]:\n",
    "               try:\n",
    "                    triplet = triplet.strip('][\\\"')\n",
    "                    if len(triplet.split('|')) == 3:\n",
    "                         row = [page_id, sec_id] + triplet.split('|')\n",
    "               except Exception as e:\n",
    "                    errors.append(e)\n",
    "               else:\n",
    "                    with open(filepath, 'a') as f:\n",
    "                         writer = csv.writer(f)\n",
    "                         writer.writerow(row)\n",
    "     if errors:\n",
    "          message = (f\"The triplets for (page: {page_id}, section: {sec_id})\"\n",
    "                     f\" could not be unpacked. Check the formatting.\")    \n",
    "          \n",
    "          print(message)\n",
    "\n",
    "\n",
    "with open(filepath, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"page title\", \"section number\", \"subject\", \"relation\", \"object\"])\n",
    "\n",
    "for page in data:\n",
    "     for section in page[\"content\"]:\n",
    "          write_triplets_to_csv(section=page[\"content\"][section], page_id=page[\"page_title\"], sec_id=section)\n",
    "\n",
    "#t.append([page[\"page_title\"], section] + list(triplet.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data_pd = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page title</th>\n",
       "      <th>section number</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atom</td>\n",
       "      <td>1.1</td>\n",
       "      <td>matter</td>\n",
       "      <td>is made up of</td>\n",
       "      <td>tiny indivisible particles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Atom</td>\n",
       "      <td>1.1</td>\n",
       "      <td>idea</td>\n",
       "      <td>is derived from</td>\n",
       "      <td>Greek word atomos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Atom</td>\n",
       "      <td>1.1</td>\n",
       "      <td>idea</td>\n",
       "      <td>appeared in</td>\n",
       "      <td>ancient cultures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Atom</td>\n",
       "      <td>1.1</td>\n",
       "      <td>idea</td>\n",
       "      <td>was based in</td>\n",
       "      <td>philosophical reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Atom</td>\n",
       "      <td>1.1</td>\n",
       "      <td>atomic theory</td>\n",
       "      <td>is not based on</td>\n",
       "      <td>old concepts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>Atom</td>\n",
       "      <td>5.3.2</td>\n",
       "      <td>[\"theories of baryogenesis</td>\n",
       "      <td>may offer</td>\n",
       "      <td>an explanation\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>Atom</td>\n",
       "      <td>5.3.2</td>\n",
       "      <td>[\"no antimatter atoms</td>\n",
       "      <td>have been discovered</td>\n",
       "      <td>in nature\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>Atom</td>\n",
       "      <td>5.3.2</td>\n",
       "      <td>[\"antimatter counterpart of hydrogen atom</td>\n",
       "      <td>was synthesized at</td>\n",
       "      <td>CERN laboratory in Geneva\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>Atom</td>\n",
       "      <td>5.3.2</td>\n",
       "      <td>[\"electron</td>\n",
       "      <td>can be replaced by</td>\n",
       "      <td>muon\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Atom</td>\n",
       "      <td>5.3.2</td>\n",
       "      <td>[\"muonic atom</td>\n",
       "      <td>can be used to test</td>\n",
       "      <td>fundamental predictions of physics\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    page title section number                                    subject  \\\n",
       "0         Atom            1.1                                     matter   \n",
       "1         Atom            1.1                                       idea   \n",
       "2         Atom            1.1                                       idea   \n",
       "3         Atom            1.1                                       idea   \n",
       "4         Atom            1.1                              atomic theory   \n",
       "..         ...            ...                                        ...   \n",
       "486       Atom          5.3.2                 [\"theories of baryogenesis   \n",
       "487       Atom          5.3.2                      [\"no antimatter atoms   \n",
       "488       Atom          5.3.2  [\"antimatter counterpart of hydrogen atom   \n",
       "489       Atom          5.3.2                                 [\"electron   \n",
       "490       Atom          5.3.2                              [\"muonic atom   \n",
       "\n",
       "                 relation                                object  \n",
       "0           is made up of            tiny indivisible particles  \n",
       "1         is derived from                     Greek word atomos  \n",
       "2             appeared in                      ancient cultures  \n",
       "3            was based in               philosophical reasoning  \n",
       "4         is not based on                          old concepts  \n",
       "..                    ...                                   ...  \n",
       "486             may offer                      an explanation\"]  \n",
       "487  have been discovered                           in nature\"]  \n",
       "488    was synthesized at           CERN laboratory in Geneva\"]  \n",
       "489    can be replaced by                                muon\"]  \n",
       "490   can be used to test  fundamental predictions of physics\"]  \n",
       "\n",
       "[491 rows x 5 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikiKG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
